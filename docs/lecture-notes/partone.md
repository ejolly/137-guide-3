
# Part 1 Lecture Notes
**Lectures 1–6 (Sep 29 – Oct 13, 2025)**

> **Scope.** You're responsible for material covered **in lecture** only (including any studies we mentioned!). Optional readings are included only for additional context

## 1) Mon 09‑29 — **A Walk Through History + Foundations of Mind Perception**
**Big Questions.** How did we get from “ignore the mind” to **studying mental representations** and predictions in social cognition? What early ideas set up modern topics like **animacy, intentional stance,** and **mentalizing**?

### Key Ideas
- Dualism (Descartes): Mind vs body distinction; body = reflexive & lawful; mind = unobservable, willful, uniquely human. Sets up later debates about **what counts as ‘mind’.**
- Empiricism (Locke): Mind as **tabula rasa**; associations learned from experience → early algorithmic flavor (associative “laws”).
- Behaviorism (Pavlov, Watson, Skinner):** Focus on observable behavior and conditioning; mind as black box; legacy in **advertising** and “psychological engineering”.
- Cognitive Revolution: From behaviorism’s **what** to cognitive science’s **how** (representations, algorithms); **how we infer minds**.
- Predictivism: representations are **predictive** in nature; they help us figure out what's going to happen **next**

---

## 2) Wed 10‑01 — **Mind Perception**
**Big Questions.** When do we **treat things as if they have a mind** and why? How do people carve minds into **capacities** that help prediction and moral judgment?

### Key Ideas
- **Heider & Simmel (1944):** With moving shapes, observers irresistibly tell mentalistic stories—**automatic attribution** of inner states beyond raw motion. (You did this in class!).
- **Intentional Stance:** Strategy to predict behavior by assuming **beliefs, desires, rationality.** We adopt it **by default** and then refine based on evidence.
- **Who has a mind?** (Turing test framing; “if you can’t tell it’s not human”). But stance adoption depends on **cues and context**; we must “be scientists and collect data.”
- **Dimensions of Mind Perception (Gray et al., 2007):**  
  - **Agency** = capacity to **plan/act** (self‑control, memory, planning).  
  - **Experience** = capacity to **feel** (pain, fear, pleasure, hunger).  
  People dissociate targets along these axes (e.g., robots vs babies vs patients in vegetative states).
- **In‑class discussion** emphasized how students placed **babies, chimps, robots, patients** differently on **experience vs agency**. Know concrete examples.
- **Predictive Mind (setup):** Representations help us **predict transitions** among **feelings** and **actions**; we’ll revisit this with Tamir/Thornton.
- Sets up **Animacy** (Lecture 3) and the idea that **seeing a mind** is an nference that guides prediction and moral evaluation.

---

## 3) Fri 10‑03 — **Animacy I**
**Big Questions.** What is our **“life detector”** tracking? What **cues** drive early, fast inferences that **something has a mind**?

### Key Ideas
- **Perception ↔ Attribution link:** You **don’t** just see a triangle—you **ask** “is there more than a triangle here?”—linking perception to **attribution** in real time.
- **Animacy:** An internal **social signal/schema** that answers “**Is it alive? Can it act? Can it feel?**” It’s **invisible** (a representation), updated from **visible** evidence. (Aristotle’s “soul is motion” cited for historical context.). Core cues:
- **Biological Motion:** Movement suggestive of **causes beyond physics**; classic **Johansson point‑light** person and **chasing** displays.
- **Cross‑modal evidence:** The **same** animacy/intentionality structure appears across **vision and sound** (e.g., music/animation mappings).
- We **search** for intentionality; brain activity tracks this search even when intention isn’t there.
- Sets up **Lecture 4**: additional cues (**rationality/efficiency**, **contingency/interaction**), **Kiki–Bouba**, and **a third perceptual pathway** specialized for **Is it animate?**

---

## 4) Wed 10‑08 — **Animacy II → Theory‑of‑Mind I: Implict Theories**
**Big Questions.** Beyond biological motion, **what other cues** trigger animacy? How do cross‑modal mappings support **shared social structure**? What happens when the life‑detector **misfires**? How do we **define ToM** formally?

### Key Ideas
- **Three evidence streams for animacy:**  
  **(a) Biological motion**, **(b) Rationality/Efficiency** (goal‑directed, shortest‑path, purposive movement), **(c) Contingency/Interaction** (interdependence among objects; e.g., **chasing**).
- **Cross‑modal mappings:** Kiki–Bouba (shape–sound mapping) shows **consistent cross‑modal correspondences**; modern versions extend to emotion & motion in **music and animation**; the brain extracts a **common dynamic** structure.
- **A third perceptual pathway for social perception:** Beyond classic **what/where**, a system specialized for **“Is it animate?”** and social cues (pSTS/STS‑adjacent systems; conceptually framed in class).
- **When animacy goes wrong:** **Uncanny Valley** = negative affect to near‑human but imperfect agents; framed as **life‑detector conflict**.
- **When mentalizing goes wrong:** **Dehumanization** = **denying mental states** to agents who deserve them. Two forms covered: **Animalistic (denial of uniquely human traits)** and **Mechanistic (denial of human nature)**.
- **Theory‑of‑Mind (ToM) – Premack & Woodruff (1978):** To have a ToM is to **impute mental states** to self and others; it’s a **theory** because states are **unobservable** and the system is used to **predict behavior**.
- Sets up **ToM strategies**: **Implicit Theories**, **Causal Inference**, **Simulation** (Lectures 5–6).

---

## 5) Fri 10‑10 — **ToM I: Implicit Theories → ToM II: Causal Inference**
**Big Questions.** What **algorithms/strategies** do we actually use to infer minds? When do our fast rules **mislead** us? How is social knowledge **organized for prediction**? What **developmental** capacities scaffold ToM?

### Key Ideas

- **Strategy I: Implicit Theories** (fast rules & schemas)
   - **Heider (Naive Psychology):** People act like scientists, generating **explanations** that distinguish **reasons (internal)** from **causes (situations)**.
   - **Jones & Davis (Correspondent Inference):** We attribute intentionality when behavior appears **freely chosen** and **goal‑directed**.
   - **Kelley (Covariation Model):** We weigh **Consistency** (across contexts), **Distinctiveness** (across targets), and **Consensus** (across observers) to infer causes (e.g., why did **Eshin laugh** at a Kevin Hart show?).
   - **Fundamental Attribution Error (FAE):** Tendency to **overestimate personal/dispositional** causes and **underestimate situational** ones when explaining others’ behavior. (Covered with class scenarios & brain‑imaging tie‑in.)
   - **Predictive Implicit Knowledge** (transition structure): **we’re good at predicting transitions** between **mental states** and **actions**; social knowledge is **organized for prediction**. (Tamir et al., 2021; Tamir & Thornton, 2023, presented in lecture.)
   - **Mental‑state space (3 axes):** **Valence**, **Social Impact**, **Rationality**; **Action space (ACT‑FAST)** (e.g., **Abstractness, Creation, Tradition, Food, Animacy, Spiritualism**). **People predict “what comes next”** by moving through these learned geometries.
- **Strategy II: Causal Inference** (developmental/evolutionary building blocks)
   - **Core systems → concepts:** Before language, infants show **intuitive physics** and **object knowledge** (e.g., **object permanence**).
   - **Violation‑of‑Expectation (VOE) paradigm:** If infants/agents look **longer** at **impossible** events, it reveals **expectations** → **proto‑beliefs**.
   - **Pretense & Decoupling (Leslie, 1987):** Ability to **represent the world differently** from what is perceived—foundational for **perspective‑taking** and later **false‑belief** understanding.
- Bridges to **Lecture 6** (Simulation),

---

## 6) Mon 10‑13 — **ToM II: Causal Inference → ToM III: Simulation**
**Big Questions.** How do we use **our own mind and body** to model others? Where do we hit **limits** in **recursive reasoning**? How do people **coordinate** in time?

### Key Ideas
- **Causal Inference → Recursive Reasoning**
- **Recursive/iterated reasoning:** *We think, they think, we think…* 
  - Most humans can’t go **deeper than ~4–5 levels**.
  - “*2/3 of the average* game” (beauty‑contest) game illustrated **k‑level** distributions
  - **Nash equilibrium** (0) is rarely chosen
  - We're **imperfect information processors**; we have **bounded rationality** 
- **Strategy III: Simulation**: using **your own thoughts/feelings/actions** to infer others’ internal states ("*how would I feel? what would I do?*); two types
   1) **Mirroring (Embodied Simulation):** Automatic vicarious **motor & affective** activations when observing others’ actions/states. Useful for **online, cue‑rich** contexts.
   2) **Self‑Projection (Internal Simulation):** **Autobiographical/episodic** construction of imagined perspectives; we can mentally travel to **past/future/counterfactual** situations and **step into someone’s shoes**. Linked to **default network** functions (memory, imagination, navigation).
- Simulation supports **mutual adaptation**
   - **Synchrony** (coupling in time & state; e.g., tapping, mimicry)
   - **Anticipatory coordination** (coupling across distinct times; e.g., dancing), 
   - **Complementarity** (different internal states; e.g., conversation, teamwork). 
   - **We simulate to predict and adapt.**
