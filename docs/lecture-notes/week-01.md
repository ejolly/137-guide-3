# Part 1 - Week 1 Lecture Notes

## Mon-09‑29 History
**Big Questions.** How did we get from “ignore the mind” to **studying mental representations** and predictions in social cognition? What early ideas set up modern topics like **animacy, intentional stance,** and **mentalizing**?

### Key Ideas
- Dualism (Descartes): Mind vs body distinction; body = reflexive & lawful; mind = unobservable, willful, uniquely human. Sets up later debates about **what counts as ‘mind’.**
- Empiricism (Locke): Mind as **tabula rasa**; associations learned from experience → early algorithmic flavor (associative “laws”).
- Behaviorism (Pavlov, Watson, Skinner):** Focus on observable behavior and conditioning; mind as black box; legacy in **advertising** and “psychological engineering”.
- Cognitive Revolution: From behaviorism’s **what** to cognitive science’s **how** (representations, algorithms); **how we infer minds**.
- Predictivism: representations are **predictive** in nature; they help us figure out what's going to happen **next**

### Lecture Notes

- **A Walk Through History: How we got to “social cognition”**
  - **Dualism → Empiricism → Behaviorism → Cognitivism → Cognitive Science → Constructivism → Computational Cognitive Science → Predictive Mind** 
    - **Dualism (Descartes)**: mind/soul distinct from body; body obeys natural laws and supports reflexes; complex conscious behavior and free will treated as uniquely human, not directly observable. (Sets up the later split between *visible* behavior vs *invisible* mind.)
    - **Empiricism (Locke)**: blank slate (tabula rasa); knowledge via sensory experience; *laws of association* (time/space/similarity) form layered associations—historical seeds of representational/learning views (and a proto‑AI flavor).
    - **Behaviorism (Pavlov, Skinner, Watson)**:
      - Pavlov’s **classical conditioning**; Skinner’s **operant conditioning** (reward/punishment control behavior; treat the mind as a black‑box). Watson pushes conditioning to attitudes/emotions (e.g., Little Albert).
      - Cultural legacy → **advertising** as “psychological engineering”: shaping liking, staying, buying via contingencies; instructive but incomplete as a total account of mind.
    - **Pivot to Cognitivism/Cognitive Psychology**:
      - We don’t just respond to stimuli; we hold **schemas, attributions, impressions**; mind as **information‑processing** using **representations**; **constructivist** view: we **construct reality** using those representations.
      - **Kurt Lewin’s Field Theory**: \(B = f(P, E)\) — behavior is a function of both person and environment (sets up our constant attention to the *percept* vs *context* tension in attribution).
    - **Computational Cognitive Science → Predictive Mind**:
      - **Cognition ≈ computation on representations**; the point of representing/computing is **to predict** the future (1990s‑present). The *predictive mind* compares top‑down predictions with sensory input; learning shapes representations; those same representations support **planning, imagining, remembering**, and—critically for this class—**simulation of other minds**.
  - **Marr’s Three Levels (bridge the “symbolic vs biology” impasse)**
    - Computational (what & why), Algorithmic/Representational (how/with what code), Implementational (neural machinery); one problem can be validly answered at multiple levels (e.g., neuron firing vs classroom behavior). This resolves a mid‑century stall: we can study representations/algorithms *and* their implementation.
- **Foundations of Mind Perception (preview)**
  - Introduces the central tension of the course: the **visible** (percepts, behavior) vs the **invisible** (unobserved internal states), and the information‑processing machinery we use to **infer** the latter from the former. (Foreshadows mentalizing, animacy, ToM.)


---

## Wed-10‑01 Mind Perception
**Big Questions.** When do we **treat things as if they have a mind** and why? How do people carve minds into **capacities** that help prediction and moral judgment?

### Key Ideas
- **Heider & Simmel (1944):** With moving shapes, observers irresistibly tell mentalistic stories—**automatic attribution** of inner states beyond raw motion. (You did this in class!).
- **Intentional Stance:** Strategy to predict behavior by assuming **beliefs, desires, rationality.** We adopt it **by default** and then refine based on evidence.
- **Who has a mind?** (Turing test framing; “if you can’t tell it’s not human”). But stance adoption depends on **cues and context**; we must “be scientists and collect data.”
- **Dimensions of Mind Perception (Gray et al., 2007):**  
  - **Agency** = capacity to **plan/act** (self‑control, memory, planning).  
  - **Experience** = capacity to **feel** (pain, fear, pleasure, hunger).  
  People dissociate targets along these axes (e.g., robots vs babies vs patients in vegetative states).
- **In‑class discussion** emphasized how students placed **babies, chimps, robots, patients** differently on **experience vs agency**. Know concrete examples.
- **Predictive Mind (setup):** Representations help us **predict transitions** among **feelings** and **actions**; we’ll revisit this with Tamir/Thornton.
- Sets up **Animacy** (Lecture 3) and the idea that **seeing a mind** is an nference that guides prediction and moral evaluation.

### Lecture Notes

- **History recap, then the predictive lens**
  - Slide overview reprises the historical arc (above), explicitly culminating in the **Predictive Mind** as a unifying way to think about perception, memory, planning, and mental simulation.
  - **Key predictive claims emphasized**:
    - The mind **generates predictions** and compares them to sensory inputs; **learning** shapes the representational code used for prediction; **action** is scaffolded by those representations; **imagination/remembering** are the same representational workspace (we will later show this “common system” also supports simulating others’ minds).
- **Social‑cognitive problem statement (framed for later units)**
  - Our job is to **predict** other agents from **incomplete/noisy** percepts by **inferring** their **latent** mental states (beliefs, desires, goals) and then **anticipating** behavior. This is the visible → invisible mapping that drives the course structure (Animacy → Theory of Mind strategies).

---

## Fri-10‑03 Animacy
**Big Questions.** What is our **“life detector”** tracking? What **cues** drive early, fast inferences that **something has a mind**?

### Key Ideas
- **Perception ↔ Attribution link:** You **don’t** just see a triangle—you **ask** “is there more than a triangle here?”—linking perception to **attribution** in real time.
- **Animacy:** An internal **social signal/schema** that answers “**Is it alive? Can it act? Can it feel?**” It’s **invisible** (a representation), updated from **visible** evidence. (Aristotle’s “soul is motion” cited for historical context.). Core cues:
- **Biological Motion:** Movement suggestive of **causes beyond physics**; classic **Johansson point‑light** person and **chasing** displays.
- **Cross‑modal evidence:** The **same** animacy/intentionality structure appears across **vision and sound** (e.g., music/animation mappings).
- We **search** for intentionality; brain activity tracks this search even when intention isn’t there.
- Sets up **Lecture 4**: additional cues (**rationality/efficiency**, **contingency/interaction**), **Kiki–Bouba**, and **a third perceptual pathway** specialized for **Is it animate?**

### Lecture Notes

- **Intentional Stance & Heider–Simmel as the doorway to mind perception**
  - **Intentional stance**: the **propensity to treat a system as if it has intentions** (beliefs, desires, goals) because that’s the **simplest way to predict** its behavior. The classic **Heider & Simmel (1944)** animation shows how irresistibly we go beyond pixels to **attribute inner states** and **generate explanations** (“big triangle bullies, small triangle escapes…”).
  - **Mentalizing** (definition used repeatedly in class): the **automatic attribution of mental states**—you supply the **best‑explaining** desire/goal/belief for what you see; perception ↔ attribution are tightly coupled.
  - **Agency vs Experience** (Gray et al.) introduced as a useful two‑dimensional *description* of attributions: **Agency** (can it act/think?) and **Experience** (can it feel?) — these axes help describe our attributions, especially when we later discuss dehumanization.
- **Why this automatic thing? The “Is it alive?” question and the Animacy signal**
  - **Animacy** is taught as an **internal social signal/schema** — a *life detector* that tracks whether a thing **can have mental states** (capable of thinking/feeling); Aristotle’s “**soul is motion**” is referenced to ground the historical intuition.
  - Lecture diagrams emphasize: we only observe the **visible**; the **invisible** (mental states) is **inferred** by computing an **internal** animacy signal from the evidence we can see.
- **Perceptual **evidence/cues** for Animacy (three major classes)**
  - **Biological motion**  
    - Motion patterns that **cannot be explained by simple physics** (e.g., point‑light walker): trajectories imply a structured body plan and self‑generated control; brains “light up” looking for a **mind** even when the dots are random—**mind‑searching** more than stimulus following.
  - **Rationality/Efficiency (Goal‑directedness)**  
    - We sense **self‑propulsion** and **efficient pathing** toward a goal (e.g., “escape” without contact): not just **violating physics**, but doing so **reasonably** with a **goal in mind**. This moves us from “odd motion” to **agency**.
  - **Contingency/Interdependence across objects**  
    - **Coordinated change** in direction/speed as objects respond to each other (e.g., **chasing/fleeing** illusions) reads as **social contingency**, not billiard‑ball physics.
- **Cross‑modal animacy & “Kiki/Bouba” style correspondences (carried into Fri 10‑08 transcript)**
  - **Sounds ↔ shapes ↔ motion** can carry a *shared* expressive structure (e.g., **sharp** sounds feel more “spiky/Kiki,” **soft** sounds feel more “round/Bouba”)—a foundation for **cross‑modal** animacy/emotion mapping.
  - **Bo Sievers et al.**: participants **compose** short melodies/animations for target emotions; **fMRI** shows **common structure** across modalities—brains **pull out** cross‑modal dynamics (soft↔hard, slow↔fast) when perceiving “aliveness”/emotion.
- **(Preview) Third visual pathway claim**: beyond “what/where,” a pathway **specialized for animacy** (looking for biological motion, contingency, agency cues) — used to justify why animacy feels so **automatic**.
