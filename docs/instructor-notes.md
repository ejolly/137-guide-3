
# PSYC 137 — Instructor Lecture Notes (Exam 1 Coverage: 09‑29 → 10‑13)

---

## Lecture 1 — 09‑29 — *A Walk Through History* + *Foundations of Mind Perception*

- **Course & Logistics (quick)**
  - Canvas is the primary hub; slides/podcasts posted; participation via brief in-class Canvas surveys. Extra‑credit via SONA. (See syllabus details.)
- **A Walk Through History: How we got to “social cognition”**
  - **Dualism → Empiricism → Behaviorism → Cognitivism → Cognitive Science → Constructivism → Computational Cognitive Science → Predictive Mind** 
    - **Dualism (Descartes)**: mind/soul distinct from body; body obeys natural laws and supports reflexes; complex conscious behavior and free will treated as uniquely human, not directly observable. (Sets up the later split between *visible* behavior vs *invisible* mind.)
    - **Empiricism (Locke)**: blank slate (tabula rasa); knowledge via sensory experience; *laws of association* (time/space/similarity) form layered associations—historical seeds of representational/learning views (and a proto‑AI flavor).
    - **Behaviorism (Pavlov, Skinner, Watson)**:
      - Pavlov’s **classical conditioning**; Skinner’s **operant conditioning** (reward/punishment control behavior; treat the mind as a black‑box). Watson pushes conditioning to attitudes/emotions (e.g., Little Albert).
      - Cultural legacy → **advertising** as “psychological engineering”: shaping liking, staying, buying via contingencies; instructive but incomplete as a total account of mind.
    - **Pivot to Cognitivism/Cognitive Psychology**:
      - We don’t just respond to stimuli; we hold **schemas, attributions, impressions**; mind as **information‑processing** using **representations**; **constructivist** view: we **construct reality** using those representations.
      - **Kurt Lewin’s Field Theory**: \(B = f(P, E)\) — behavior is a function of both person and environment (sets up our constant attention to the *percept* vs *context* tension in attribution).
    - **Computational Cognitive Science → Predictive Mind**:
      - **Cognition ≈ computation on representations**; the point of representing/computing is **to predict** the future (1990s‑present). The *predictive mind* compares top‑down predictions with sensory input; learning shapes representations; those same representations support **planning, imagining, remembering**, and—critically for this class—**simulation of other minds**.
  - **Marr’s Three Levels (bridge the “symbolic vs biology” impasse)**
    - Computational (what & why), Algorithmic/Representational (how/with what code), Implementational (neural machinery); one problem can be validly answered at multiple levels (e.g., neuron firing vs classroom behavior). This resolves a mid‑century stall: we can study representations/algorithms *and* their implementation.
- **Foundations of Mind Perception (preview)**
  - Introduces the central tension of the course: the **visible** (percepts, behavior) vs the **invisible** (unobserved internal states), and the information‑processing machinery we use to **infer** the latter from the former. (Foreshadows mentalizing, animacy, ToM.)

---

## Lecture 2 — 10‑01 — *Foundations of Mind Perception* (recap & predictive framing)

- **History recap, then the predictive lens**
  - Slide overview reprises the historical arc (above), explicitly culminating in the **Predictive Mind** as a unifying way to think about perception, memory, planning, and mental simulation.
  - **Key predictive claims emphasized**:
    - The mind **generates predictions** and compares them to sensory inputs; **learning** shapes the representational code used for prediction; **action** is scaffolded by those representations; **imagination/remembering** are the same representational workspace (we will later show this “common system” also supports simulating others’ minds).
- **Social‑cognitive problem statement (framed for later units)**
  - Our job is to **predict** other agents from **incomplete/noisy** percepts by **inferring** their **latent** mental states (beliefs, desires, goals) and then **anticipating** behavior. This is the visible → invisible mapping that drives the course structure (Animacy → Theory of Mind strategies).

---

## Lecture 3 — 10‑03 — *Detecting Minds (Animacy) & Mentalizing Basics*

- **Intentional Stance & Heider–Simmel as the doorway to mind perception**
  - **Intentional stance**: the **propensity to treat a system as if it has intentions** (beliefs, desires, goals) because that’s the **simplest way to predict** its behavior. The classic **Heider & Simmel (1944)** animation shows how irresistibly we go beyond pixels to **attribute inner states** and **generate explanations** (“big triangle bullies, small triangle escapes…”).
  - **Mentalizing** (definition used repeatedly in class): the **automatic attribution of mental states**—you supply the **best‑explaining** desire/goal/belief for what you see; perception ↔ attribution are tightly coupled.
  - **Agency vs Experience** (Gray et al.) introduced as a useful two‑dimensional *description* of attributions: **Agency** (can it act/think?) and **Experience** (can it feel?) — these axes help describe our attributions, especially when we later discuss dehumanization.
- **Why this automatic thing? The “Is it alive?” question and the Animacy signal**
  - **Animacy** is taught as an **internal social signal/schema** — a *life detector* that tracks whether a thing **can have mental states** (capable of thinking/feeling); Aristotle’s “**soul is motion**” is referenced to ground the historical intuition.
  - Lecture diagrams emphasize: we only observe the **visible**; the **invisible** (mental states) is **inferred** by computing an **internal** animacy signal from the evidence we can see.
- **Perceptual **evidence/cues** for Animacy (three major classes)**
  - **Biological motion**  
    - Motion patterns that **cannot be explained by simple physics** (e.g., point‑light walker): trajectories imply a structured body plan and self‑generated control; brains “light up” looking for a **mind** even when the dots are random—**mind‑searching** more than stimulus following.
  - **Rationality/Efficiency (Goal‑directedness)**  
    - We sense **self‑propulsion** and **efficient pathing** toward a goal (e.g., “escape” without contact): not just **violating physics**, but doing so **reasonably** with a **goal in mind**. This moves us from “odd motion” to **agency**.
  - **Contingency/Interdependence across objects**  
    - **Coordinated change** in direction/speed as objects respond to each other (e.g., **chasing/fleeing** illusions) reads as **social contingency**, not billiard‑ball physics.
- **Cross‑modal animacy & “Kiki/Bouba” style correspondences (carried into Fri 10‑08 transcript)**
  - **Sounds ↔ shapes ↔ motion** can carry a *shared* expressive structure (e.g., **sharp** sounds feel more “spiky/Kiki,” **soft** sounds feel more “round/Bouba”)—a foundation for **cross‑modal** animacy/emotion mapping.
  - **Bo Sievers et al.**: participants **compose** short melodies/animations for target emotions; **fMRI** shows **common structure** across modalities—brains **pull out** cross‑modal dynamics (soft↔hard, slow↔fast) when perceiving “aliveness”/emotion.
- **(Preview) Third visual pathway claim**: beyond “what/where,” a pathway **specialized for animacy** (looking for biological motion, contingency, agency cues) — used to justify why animacy feels so **automatic**.

---

## Lecture 4 — 10‑08 — *From Animacy to Theory‑of‑Mind (ToM)*

- **Quick announcements (exam logistics)**
  - Challenge 1 **Fri Oct 17**; study materials and an **optional review session on Wed Oct 15**; **Monday’s class is last testable lecture**, and students should **submit questions** via that day’s Canvas assignment (used to structure Wednesday).
- **Recap: Mentalizing & Animacy (tying the schema to perception)**
  - Re‑emphasis: everything **below the line** (slides) is **perceptual evidence**; everything **above** is **invisible**; **animacy** is the internal, **computed** signal we use to bridge the two.
  - **Three evidence classes** (biological motion, rational/efficient goal‑directedness, contingency) are *not* mutually exclusive and often **co‑occur**, but can be **isolated experimentally** (e.g., self‑propulsion without contact, chasing displays).
- **When animacy goes “wrong”: The **Uncanny Valley** (amplified by motion)**
  - As a stimulus becomes **almost human** (face/body/voice), small mismatches in the **social signal** (shape, timing, contingency) evoke a **negative affective dip**; **motion exacerbates** the effect. The class connected this to current interactions with chatbots: something feels **missing**.
- **When mentalizing goes “wrong”: **Dehumanization** (Haslam)**
  - **Denying deserved mental states** along two **denial axes**:
    - **Animalistic** dehumanization: stripping what separates humans from other animals (typical hits to **rationality**, **self‑control**, etc.).
    - **Mechanistic** dehumanization: stripping **experience/agency**, treating others as **objects/machines**.
  - Mapped back to earlier **Agency/Experience** dimensions; discussion emphasized that while mentalizing is automatic, stories can be **updated** (we can **intervene** on our attributions).
- **Transition: From “Does it have a mind?” to “What state is that mind in?” → **Theory of Mind (ToM)**
  - **Definition used in class** (Premack & Woodruff framing): **capacity to represent another agent’s beliefs, desires, intentions** to **explain** and **predict** behavior; i.e., *how we generate explanations for the mental states we attribute.*
  - **Non‑human primates & infants (preview via video)**: **gaze‑based false‑belief** logic (observer looks where an agent *wrongly believes* an object to be). **Apes** (and later **preverbal infants**) show looking patterns consistent with **attributing false beliefs**, hinting ToM is **not human‑exclusive** and **pre‑verbal** reasoning is measurable with **VOE** (Violation‑of‑Expectation) methods.
- **Three broad ToM “algorithms” (introduced)**
  - **Implicit Theories** (fast rules/schemas we carry).
  - **Causal Inference** (reason backward from observation to hidden causes).
  - **Simulation** (use one’s own mind/body to model another).

---

## Lecture 5 — 10‑10 — *ToM Strategy I: Implicit Theories → (to) Causal Inference*

- **Announcements / framing**: Monday’s lecture is the **last included** for Challenge 1; **submit questions** via Monday’s Canvas assignment; Wednesday is **optional** review.
- **Implicit Theories of Mind (historical thread and the fast‑and‑frugal strategy)**
  - **Heider’s “Naive Psychology”**: people act like scientists separating **reasons** (internal motives) from **causes** (situations).
  - **Jones & Davis (Correspondent Inference)**: we infer intentions especially when behavior seems **freely chosen** (could they have done otherwise?). Free action cues → stronger mentalizing about **what they were trying to achieve**.
  - **Kelley (Covariation Model)**:
    - We implicitly track **Consistency** (across contexts), **Distinctiveness** (across targets), **Consensus** (across actors).  
    - Example used repeatedly: “Why did **Eshin laugh** at the **Kevin Hart** show?”  
      - If he laughs in general (**consistency**), only at Kevin Hart (**distinctiveness**), and others laugh too (**consensus**), we infer **he likes Kevin Hart**.
  - **Fundamental Attribution Error (FAE)** (defined & reframed):
    - The **tendency** to **overestimate personal** causes and **underestimate situational** ones when explaining others’ behavior.
    - Reframing from lecture: not merely an “error” — could be a **by‑product** of our **spontaneous tendency to see minds**. **Neuroimaging** shows stronger engagement of the same **mentalizing‑related regions** in participants who favored dispositional explanations for ambiguous vignettes (the same stories you answered in class). **We can still update** with more evidence.
- **Our social knowledge is organized for **prediction** (bridging to modern view)**
  - Daily life has **temporal structure** (e.g., **American Time Use**–style transitions: sleep → work → run…). We carry **intuitions** about **what comes next**, both for **actions** and **feelings**.
  - **Key proposal from recent work (presented in slides)**:
    - **Mental‑state space (3 axes)**: **Valence** (±), **Social Impact** (interpersonal force), **Rationality** (uniquely human/reflective).  
    - **Action space (ACT‑FAST taxonomy; 6 axes)**: **Abstractness**, **Creation**, **Tradition**, **Food**, **Animacy**, **Spiritualism**.
    - **Minds as sequences**: Brains can represent a person as the **sequence of mental states we predict they’ll experience next**; this is **useful** precisely because it helps us **anticipate** behavior.
- **(Transition) Core Systems → Causal Inference**
  - Lecture emphasizes that much **pre‑linguistic** understanding comes from **core knowledge** — notably **intuitive physics**. **Violation‑of‑Expectation (VOE)** paradigms show infants stare longer at **impossible** events (e.g., occlusion violations), evidencing **expectations** ⇒ **proto‑beliefs** about the world.
  - **Causal inference (formalized next):** the **general ability to reason backwards** from observations to causes (“Why am I seeing this?” “What did I expect to see?”).
- **In‑class game (announced at end of 10‑10 slides)**
  - “**2/3 of the average**” guessing contest (rules posted): submit one number; aim for 2/3 of class average; no talk/calculators; candy prize next time. (Results revealed in 10‑13.)

---

## Lecture 6 — 10‑13 — *ToM Strategy II: Causal Inference (recap) & Strategy III: Simulation*

- **Recap (Causal inference & development)**
  - **Intuitive physics** and **VOE** reviewed: occlusion/cart experiments (Baillargeon et al.) show infants as young as ~3.5 months **expect** object permanence and look longer at **impossible** events; **belief/expectation** signals without language.
  - **Causal inference facilitates decoupling**: understanding cause→effect supports **pretense** (Leslie, 1987) — the capacity to **represent the world differently from what it is** (“pretending”) → cognitive **decoupling** from immediate perception. This, in turn, scaffolds **representing others’ beliefs** (e.g., false‑belief).
  - **Formal slide definition**: Causal inference = **reasoning backwards** from observations to hidden causes to **predict** future behavior; provides the basis for **higher‑order** ToM.
- **Recursive reasoning (orders of intentionality) — class contest results**
  - Results from the **2/3 of the average** game:
    - **Class average** = **46.4** → **2/3** (rounded) = **31**; **four winners** named on slide. Distribution mapped onto intentionality levels:  
      - **k=0** (first‑order beliefs) ~25–30%  
      - **k=1** (second‑order) ~9%  
      - **k=2** (third‑order) ~7%  
      - **k=3** (fourth‑order) ~3%  
    - Take‑home: humans show **bounded rationality** and **finite recursion** (practically ≤ 4–5 levels), consistent with **Cognitive Hierarchy** models (Nagel; Camerer et al.).
- **ToM Strategy III: Simulation** (two mechanisms emphasized)
  - **Mirroring / Embodied simulation**  
    - Observing actions/emotions **automatically activates** **motor** and **affective** representations (partial resonance) that guide **fast inferences** about another’s state. (Waytz & Mitchell framing on slides.)
  - **Projection / Internal simulation**  
    - We **use autobiographical memory** and **imagination** to construct others’ perspectives—mentally **travel** to **future/past**, to **counterfactuals**, or into **someone else’s shoes**. The same neural system that supports **remembering** and **navigation** supports **simulating** other minds (Buckner & Carroll figure on slides).
- **Mutual adaptation via simulation → social connection**
  - **Synchrony** (time & internal‑state coupling; e.g., tapping, mimicry).  
  - **Anticipatory coordination** (coupling **across distinct times**; e.g., dancing).  
  - **Complementarity** (coupling **across distinct internal states**; e.g., conversation, teamwork).  
  - Slogan from slides: **Simulating to predict. Predicting to adapt. Adapting to connect.**
- **Grand recap via Marr’s levels (applied to the whole first unit)**
  - **Computational (what/why)**: **Detect other minds** to know **how to act** in a social world.
  - **Algorithmic (how/what’s represented)**: **Look for animacy**; **attribute mental states** using **implicit knowledge**, **causal reasoning**, and **simulation**; **minds as action↔mental‑state transitions** (predictive organization).
  - **Implementational (constraints)**: **Cross‑modal animacy cues**; **common system** for remembering/imagining/navigating/simulating; **bounded rationality** (finite recursion).

---